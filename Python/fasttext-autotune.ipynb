{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PoC Autotune Techday\n",
    "\n",
    "The main objetive of this PoC it's to perform an assessment about the [new feature of FastText for hyperparametrization for the training time called Autotune](https://fasttext.cc/docs/en/autotune.html). \n",
    "\n",
    "\n",
    "### What is Autotune?\n",
    "\n",
    "From the [press release](https://ai.facebook.com/blog/fasttext-blog-post-open-source-in-brief/) the description of Autotune is:\n",
    "\n",
    "*[...]This feature automatically determines the best hyperparameters for your data set in order to build an efficient text classifier[...].*  \n",
    "\n",
    "*[...]FastText then uses the allotted time to search for the hyperparameters that give the best performance on the validation set.[...].*  \n",
    "\n",
    "*[...]Our strategy to explore various hyperparameters is inspired by existing tools, such as Nevergrad, but tailored to fastText by leveraging the specific structure of models. Our autotune explores hyperparameters by sampling, initially in a large domain that shrinks around the best combinations found over time[...]*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The boilerplate code for data generation is in [Tutorials in the FastText documentation](https://fasttext.cc/docs/en/supervised-tutorial.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_stats(model):\n",
    "    \"\"\" Get the main stats for model and perform a \n",
    "    single test using a text.For model prediction \n",
    "    we'll use the text from docs.\n",
    "    \"\"\"\n",
    "    pred_class = model.predict(\"Which baking dish is best to bake a banana bread ?\")[0]\n",
    "    pred_proba = model.predict(\"Which baking dish is best to bake a banana bread ?\")[1]\n",
    "    precision = model.test(\"cooking.valid\")[1]\n",
    "    recall = model.test(\"cooking.valid\")[2]\n",
    "    \n",
    "    print(f'Predicted Class: {pred_class[0]}\\nPredicted Probabilily: {round(pred_proba[0] *100, 2)} %')\n",
    "    print(f'Precision: {round(precision *100, 2)} %\\nRecall: {round(recall *100, 2)} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Model\n",
    "\n",
    "The idea here it's to have a plain vanilla model baseline just to make a small comparison and see the evolution in the recall and precision along the time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised(input=\"cooking.train\",\n",
    "                                  epoch=25,\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: __label__baking\n",
      "Predicted Probabilily: 63.14 %\n",
      "Precision: 51.9 %\n",
      "Recall: 22.44 %\n"
     ]
    }
   ],
   "source": [
    "get_model_stats(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autotune Options\n",
    "\n",
    "We're going across all options for autotune and describe them. Due the lack of documentation about the feature in the main repository most of what we're going to see here was based just checking the source code in the main repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `autotuneValidationFile`\n",
    "\n",
    "`autotuneValidationFile`: Validation file that need to be passed for the autotune. Should be in the same format as training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: __label__bread\n",
      "Predicted Probabilily: 34.16 %\n",
      "Precision: 56.5 %\n",
      "Recall: 24.43 %\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_supervised(input='cooking.train',\n",
    "                                  autotuneValidationFile='cooking.valid',\n",
    "                                 )\n",
    "\n",
    "get_model_stats(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `autotuneMetric`\n",
    "\n",
    "`autotuneMetric`:  Metric used for autotune for the model assessment. Default `f1-score`. Options available:\n",
    "- `{f1}`\n",
    "- `{f1:labelname}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: __label__bread\n",
      "Predicted Probabilily: 34.53 %\n",
      "Precision: 56.43 %\n",
      "Recall: 24.41 %\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_supervised(input='cooking.train',\n",
    "                                  autotuneValidationFile='cooking.valid',\n",
    "                                  autotuneMetric=\"f1\",\n",
    "                                 )\n",
    "\n",
    "get_model_stats(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: __label__bread\n",
      "Predicted Probabilily: 40.55 %\n",
      "Precision: 51.3 %\n",
      "Recall: 22.19 %\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_supervised(input='cooking.train',\n",
    "                                  autotuneValidationFile='cooking.valid',\n",
    "                                  autotuneMetric=\"f1:__label__baking\",\n",
    "                                 )\n",
    "\n",
    "get_model_stats(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `autotuneModelSize`\n",
    "\n",
    "`autotuneModelSize`:  Desired size of the model already quantized. If the value is empty, it means \"do not quantize\". Byte units available:  \n",
    "- `{'k', 1000}`\n",
    "- `{'K', 1000}`\n",
    "- `{'m', 1000000}`\n",
    "- `{'M', 1000000}`\n",
    "- `{'g', 1000000000}`\n",
    "- `{'G', 1000000000}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: __label__baking\n",
      "Predicted Probabilily: 63.47 %\n",
      "Precision: 54.9 %\n",
      "Recall: 23.74 %\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_supervised(input='cooking.train',\n",
    "                                  autotuneValidationFile='cooking.valid',\n",
    "                                  autotuneModelSize=\"2M\",\n",
    "                                 )\n",
    "\n",
    "get_model_stats(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `autotuneDuration`\n",
    "\n",
    "`autotuneDuration`: Timespan in seconds for autotune perform the search between several options. Default `60 * 5; // 5 minutes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Class: __label__baking\n",
      "Predicted Probabilily: 32.4 %\n",
      "Precision: 22.17 %\n",
      "Recall: 9.59 %\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_supervised(input='cooking.train',\n",
    "                                  autotuneValidationFile='cooking.valid',\n",
    "                                  autotuneDuration=60,\n",
    "                                 )\n",
    "\n",
    "get_model_stats(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `autotunePredictions`\n",
    "\n",
    "`autotunePredictions`: Number of predictions used for evaluation. Default `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised(input='cooking.train',\n",
    "                                  autotuneValidationFile='cooking.valid',\n",
    "                                  autotunePredictions=200,\n",
    "                                 )\n",
    "\n",
    "get_model_stats(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combination of all parameters in Autotune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fasttext.train_supervised(input='cooking.train',\n",
    "                                  autotuneValidationFile='cooking.valid',\n",
    "                                  autotuneMetric=\"f1\",\n",
    "                                  autotuneModelSize=\"200M\",\n",
    "                                  autotuneDuration=1200,\n",
    "                                  autotunePredictions=200\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autotune Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the [code we can find the search strategy for the Autotune follows](https://github.com/facebookresearch/fastText/blob/master/src/autotune.cc):\n",
    "\n",
    "For all parameters, the Autotuner have an updater (method `updateArgGauss()`) that considers a random number provided by a Gaussian distribution function (`coeff`) and set an update number between a single standard deviation (parameters `startSigma` and `endSigma`) and based on these values the coefficients have an update.\n",
    "\n",
    "Each parameter has a specific range for the `startSigma` and `endSigma` that it's fixed in the `updateArgGauss` method. \n",
    "\n",
    "Updates for each coefficient can be `linear` (i.e. `updateCoeff + val`) or `power` (i.e. `pow(2.0, coeff); updateCoeff * val`) and depends from the first random gaussian random number that are inside of standard deviation. \n",
    "\n",
    "After each validation (that uses a different combination of parameters) one score (f1-score only) it's stored and the best one will be used to train the full model using the best combination of parameters. \n",
    "\n",
    "**Arguments Range**\n",
    "\n",
    "- `epoch`: `1` to `100`\n",
    "- `learning rate`: `0.01` to `5.00`\n",
    "- `dimensions`: `1` to `1000`\n",
    "- `wordNgrams`: `1` to `5`\n",
    "- `loss`: Only `softmax`\n",
    "- `bucket size`: `10000` to `10000000`\n",
    "- `minn` (min length of char ngram): `1` to `3`\n",
    "- `maxn` (max length of char ngram): `1` to `minn + 3`\n",
    "- `dsub` (size of each sub-vector): `1` to `4`\n",
    "\n",
    "Clarification posted in [issues in FastText project](https://github.com/facebookresearch/fastText/issues/891).\n",
    "\n",
    "\n",
    "In terms of metrics for optimization there's only the `f1score` and `labelf1score` metrics.\n",
    "\n",
    "[Doubt about only two metrics](https://github.com/facebookresearch/fastText/issues/892)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation in C in a simple Python representation it seems like this (to be reviewed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Fixed Sigma ranges\n",
    "startSigma = 2.8\n",
    "endSigma = 2.5\n",
    "\n",
    "# Time parameter (remaining time)\n",
    "t = 10\n",
    "\n",
    "# Epochs\n",
    "epoch = 35\n",
    "\n",
    "# Standard deviation\n",
    "stddev = startSigma -((startSigma - endSigma) / 0.5) * min(0.5, max((t - 0.25), 0.0))\n",
    "\n",
    "# Coefficient update\n",
    "mu = 0.0\n",
    "sigma = stddev\n",
    "\n",
    "# Number for update\n",
    "coeff = np.random.normal(mu, sigma)\n",
    "print(f'Coefficient value: {coeff}')\n",
    "\n",
    "# Coefficient Update\n",
    "print(f'Coefficient Update value: {np.power(2, coeff)}')\n",
    "\n",
    "# Coefficient Update\n",
    "print(f'Update Epochs: {(np.power(2, coeff)) + epoch}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

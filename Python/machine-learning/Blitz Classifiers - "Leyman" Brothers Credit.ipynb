{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blitz Classifiers (v0.01)\n",
    "\n",
    "Here we will present Blitz Classifiers in Scikit-Learn.\n",
    "\n",
    "The main idea here is to use a simple concept to choose the best algorithm that fit in your data. \n",
    "\n",
    "Note the main funciton of Blitz Classifiers it's to simplify the initial algorithm and after that, you as a Machine Learning Engineer can choose the best algorithm that solve your problem considering complexity, scalability and knowledge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First at all, let's import some useful libraries.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/flavio.clesio/anaconda/lib/python3.5/site-packages/pandas/computation/__init__.py:19: UserWarning: The installed version of numexpr 2.4.4 is not supported in pandas and will be not be used\n",
      "\n",
      "  UserWarning)\n",
      "/Users/flavio.clesio/anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import cross_validation\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this time we'll import the following classifiers of scikit-learn:\n",
    "- Random Forest\n",
    "- Gradient Boosting\n",
    "- Extra Trees\n",
    "- AdaBoost\n",
    "- SVC\n",
    "- KNeighbors\n",
    "- Decision Tree\n",
    "- Perceptron\n",
    "- Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll import a structured dataset that all columns are numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "credit = pd.read_csv('https://raw.githubusercontent.com/fclesio/learning-space/master/Datasets/02%20-%20Classification/default_credit_card.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see out dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>DEFAULT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>20000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>689</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>120000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3272</td>\n",
       "      <td>3455</td>\n",
       "      <td>3261</td>\n",
       "      <td>0</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>90000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>14331</td>\n",
       "      <td>14948</td>\n",
       "      <td>15549</td>\n",
       "      <td>1518</td>\n",
       "      <td>1500</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>50000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28314</td>\n",
       "      <td>28959</td>\n",
       "      <td>29547</td>\n",
       "      <td>2000</td>\n",
       "      <td>2019</td>\n",
       "      <td>1200</td>\n",
       "      <td>1100</td>\n",
       "      <td>1069</td>\n",
       "      <td>1000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>50000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>20940</td>\n",
       "      <td>19146</td>\n",
       "      <td>19131</td>\n",
       "      <td>2000</td>\n",
       "      <td>36681</td>\n",
       "      <td>10000</td>\n",
       "      <td>9000</td>\n",
       "      <td>689</td>\n",
       "      <td>679</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0   1      20000    2          2         1   24      2      2     -1     -1   \n",
       "1   2     120000    2          2         2   26     -1      2      0      0   \n",
       "2   3      90000    2          2         2   34      0      0      0      0   \n",
       "3   4      50000    2          2         1   37      0      0      0      0   \n",
       "4   5      50000    1          2         1   57     -1      0     -1      0   \n",
       "\n",
       "    ...     BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0   ...             0          0          0         0       689         0   \n",
       "1   ...          3272       3455       3261         0      1000      1000   \n",
       "2   ...         14331      14948      15549      1518      1500      1000   \n",
       "3   ...         28314      28959      29547      2000      2019      1200   \n",
       "4   ...         20940      19146      19131      2000     36681     10000   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  DEFAULT  \n",
       "0         0         0         0        1  \n",
       "1      1000         0      2000        1  \n",
       "2      1000      1000      5000        0  \n",
       "3      1100      1069      1000        0  \n",
       "4      9000       689       679        0  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we have only numerical attributes. Below, let's see some correlations with our dependent variable (DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID          -0.013952\n",
       "LIMIT_BAL   -0.153520\n",
       "SEX         -0.039961\n",
       "EDUCATION    0.028006\n",
       "MARRIAGE    -0.024339\n",
       "AGE          0.013890\n",
       "PAY_0        0.324794\n",
       "PAY_2        0.263551\n",
       "PAY_3        0.235253\n",
       "PAY_4        0.216614\n",
       "PAY_5        0.204149\n",
       "PAY_6        0.186866\n",
       "BILL_AMT1   -0.019644\n",
       "BILL_AMT2   -0.014193\n",
       "BILL_AMT3   -0.014076\n",
       "BILL_AMT4   -0.010156\n",
       "BILL_AMT5   -0.006760\n",
       "BILL_AMT6   -0.005372\n",
       "PAY_AMT1    -0.072929\n",
       "PAY_AMT2    -0.058579\n",
       "PAY_AMT3    -0.056250\n",
       "PAY_AMT4    -0.056827\n",
       "PAY_AMT5    -0.055124\n",
       "PAY_AMT6    -0.053183\n",
       "DEFAULT      1.000000\n",
       "Name: DEFAULT, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credit.corr()[\"DEFAULT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "in that part of the code, we'll select the features of our dataset to split the dataset in test and train sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = credit.columns[1:24]\n",
    "target = credit.columns[24:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X_train: independent (target) variables for training data set\n",
    "# Y_train: dependent (outcome) variable for training data set\n",
    "\n",
    "# X_test: independent (target) variables for the test data set\n",
    "# Y_test: dependent (outcome) variable for the test data set\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(\n",
    "    credit[features].values, credit['DEFAULT'].values, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the shape of our datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24000, 23)\n",
      "(6000, 23)\n",
      "(24000,)\n",
      "(6000,)\n"
     ]
    }
   ],
   "source": [
    "print (X_train.shape)\n",
    "print (X_test.shape)\n",
    "print (Y_train.shape)\n",
    "print (Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll instance our objects with the classifiers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=100, min_samples_leaf=10, random_state=1, n_jobs=2)\n",
    "gbc = GradientBoostingClassifier()\n",
    "etc = ExtraTreesClassifier()\n",
    "abc = AdaBoostClassifier()\n",
    "svc = SVC()\n",
    "knc = KNeighborsClassifier()\n",
    "dtc = DecisionTreeClassifier()\n",
    "ptc = Perceptron()\n",
    "lrc = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the training sets, we'll fit all models for each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(X_train, Y_train)\n",
    "gbc.fit(X_train, Y_train)\n",
    "etc.fit(X_train, Y_train)\n",
    "abc.fit(X_train, Y_train)\n",
    "svc.fit(X_train, Y_train)\n",
    "knc.fit(X_train, Y_train)\n",
    "dtc.fit(X_train, Y_train)\n",
    "ptc.fit(X_train, Y_train)\n",
    "lrc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll build an object called _expected_ with our target variables of training set. We'll use this to see the adherence of the model and see the errors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expected = Y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use the _predict_ method over our training atributes to build every prediction object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predicted_rfc = rfc.predict(X_train)\n",
    "predicted_gbc = gbc.predict(X_train)\n",
    "predicted_etc = etc.predict(X_train)\n",
    "predicted_abc = abc.predict(X_train)\n",
    "predicted_svc = svc.predict(X_train)\n",
    "predicted_knc = knc.predict(X_train)\n",
    "predicted_dtc = dtc.predict(X_train)\n",
    "predicted_ptc = ptc.predict(X_train)\n",
    "predicted_lrc = lrc.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you feel confortable to see every classification report, feel free to execute this code below (will be deprecated in next version). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.97      0.91     18661\n",
      "          1       0.80      0.44      0.57      5339\n",
      "\n",
      "avg / total       0.84      0.85      0.83     24000\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.84      0.95      0.89     18661\n",
      "          1       0.70      0.38      0.49      5339\n",
      "\n",
      "avg / total       0.81      0.83      0.80     24000\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     18661\n",
      "          1       1.00      1.00      1.00      5339\n",
      "\n",
      "avg / total       1.00      1.00      1.00     24000\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.96      0.89     18661\n",
      "          1       0.68      0.32      0.43      5339\n",
      "\n",
      "avg / total       0.80      0.82      0.79     24000\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00     18661\n",
      "          1       1.00      0.98      0.99      5339\n",
      "\n",
      "avg / total       0.99      0.99      0.99     24000\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.83      0.95      0.89     18661\n",
      "          1       0.67      0.34      0.45      5339\n",
      "\n",
      "avg / total       0.80      0.82      0.79     24000\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     18661\n",
      "          1       1.00      1.00      1.00      5339\n",
      "\n",
      "avg / total       1.00      1.00      1.00     24000\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.44      0.57     18661\n",
      "          1       0.25      0.67      0.37      5339\n",
      "\n",
      "avg / total       0.69      0.49      0.52     24000\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.78      1.00      0.87     18661\n",
      "          1       0.00      0.00      0.00      5339\n",
      "\n",
      "avg / total       0.60      0.78      0.68     24000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.classification_report(expected, predicted_rfc))\n",
    "print(metrics.classification_report(expected, predicted_gbc))\n",
    "print(metrics.classification_report(expected, predicted_etc))\n",
    "print(metrics.classification_report(expected, predicted_abc))\n",
    "print(metrics.classification_report(expected, predicted_svc))\n",
    "print(metrics.classification_report(expected, predicted_knc))\n",
    "print(metrics.classification_report(expected, predicted_dtc))\n",
    "print(metrics.classification_report(expected, predicted_ptc))\n",
    "print(metrics.classification_report(expected, predicted_lrc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same above applies for the confusion matrix for each classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18083   578]\n",
      " [ 3006  2333]]\n",
      "[[17776   885]\n",
      " [ 3314  2025]]\n",
      "[[18660     1]\n",
      " [    8  5331]]\n",
      "[[17884   777]\n",
      " [ 3656  1683]]\n",
      "[[18642    19]\n",
      " [  125  5214]]\n",
      "[[17785   876]\n",
      " [ 3542  1797]]\n",
      "[[18660     1]\n",
      " [    8  5331]]\n",
      "[[ 8136 10525]\n",
      " [ 1777  3562]]\n",
      "[[18659     2]\n",
      " [ 5339     0]]\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(expected, predicted_rfc))\n",
    "print(metrics.confusion_matrix(expected, predicted_gbc))\n",
    "print(metrics.confusion_matrix(expected, predicted_etc))\n",
    "print(metrics.confusion_matrix(expected, predicted_abc))\n",
    "print(metrics.confusion_matrix(expected, predicted_svc))\n",
    "print(metrics.confusion_matrix(expected, predicted_knc))\n",
    "print(metrics.confusion_matrix(expected, predicted_dtc))\n",
    "print(metrics.confusion_matrix(expected, predicted_ptc))\n",
    "print(metrics.confusion_matrix(expected, predicted_lrc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll predict with our test dataset to see the adherence of our models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions_rfc = rfc.predict(X_test)\n",
    "predictions_gbc = gbc.predict(X_test)\n",
    "predictions_etc = etc.predict(X_test)\n",
    "predictions_abc = abc.predict(X_test)\n",
    "predictions_svc = svc.predict(X_test)\n",
    "predictions_knc = knc.predict(X_test)\n",
    "predictions_dtc = dtc.predict(X_test)\n",
    "predictions_ptc = ptc.predict(X_test)\n",
    "predictions_lrc = lrc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store our Mean Squared Error for each classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mse_rfc = mean_squared_error(predictions_rfc, Y_test)\n",
    "mse_abc = mean_squared_error(predictions_abc, Y_test)\n",
    "mse_etc = mean_squared_error(predictions_etc, Y_test)\n",
    "mse_gbc = mean_squared_error(predictions_gbc, Y_test)\n",
    "mse_svc = mean_squared_error(predictions_svc, Y_test)\n",
    "mse_knc = mean_squared_error(predictions_knc, Y_test)\n",
    "mse_dtc = mean_squared_error(predictions_dtc, Y_test)\n",
    "mse_ptc = mean_squared_error(predictions_ptc, Y_test)\n",
    "mse_lrc = mean_squared_error(predictions_lrc, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE - Random Forests: 0.172\n",
      "RMSE - Gradient Boosting: 0.172\n",
      "RMSE - Extra Trees: 0.195\n",
      "RMSE - Ada Boosting: 0.174\n",
      "RMSE - SVM: 0.216\n",
      "RMSE - KNN: 0.238\n",
      "RMSE - Decision Trees: 0.263\n",
      "RMSE - Perceptron: 0.518\n",
      "RMSE - Logistic Regression: 0.216\n"
     ]
    }
   ],
   "source": [
    "print('RMSE - Random Forests:',round(mse_rfc,3) )\n",
    "print('RMSE - Gradient Boosting:',round(mse_gbc,3) )\n",
    "print('RMSE - Extra Trees:',round(mse_etc,3) )\n",
    "print('RMSE - Ada Boosting:',round(mse_abc,3) )\n",
    "print('RMSE - SVM:',round(mse_svc,3) )\n",
    "print('RMSE - KNN:',round(mse_knc,3) )\n",
    "print('RMSE - Decision Trees:',round(mse_dtc,3) )\n",
    "print('RMSE - Perceptron:',round(mse_ptc,3) )\n",
    "print('RMSE - Logistic Regression:',round(mse_lrc,3) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's ranking our algorithms to see the best one to start our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gradient Boosting</td>\n",
       "      <td>0.1720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forests</td>\n",
       "      <td>0.1723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ada Boosting</td>\n",
       "      <td>0.1740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Extra Trees</td>\n",
       "      <td>0.1947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.2158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.2160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.2377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Decision Trees</td>\n",
       "      <td>0.2632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Perceptron</td>\n",
       "      <td>0.5178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Algorithm     MSE\n",
       "1    Gradient Boosting  0.1720\n",
       "0       Random Forests  0.1723\n",
       "3         Ada Boosting  0.1740\n",
       "2          Extra Trees  0.1947\n",
       "4                  SVM  0.2158\n",
       "8  Logistic Regression  0.2160\n",
       "5                  KNN  0.2377\n",
       "6       Decision Trees  0.2632\n",
       "7           Perceptron  0.5178"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algorithms = {'Algorithm': ['Random Forests', 'Gradient Boosting', 'Extra Trees', 'Ada Boosting', 'SVM', 'KNN', 'Decision Trees', 'Perceptron', 'Logistic Regression'],\n",
    "        'MSE': [round(mse_rfc,4), round(mse_gbc,4), round(mse_etc,4), round(mse_abc,4), round(mse_svc,4), round(mse_knc,4), round(mse_dtc,4), round(mse_ptc,4), round(mse_lrc,4)]}\n",
    "\n",
    "# Transform in a data frame of Pandas to sorting\n",
    "algos = pd.DataFrame(algorithms)\n",
    "\n",
    "algos.sort_values(by='MSE', ascending=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the Gradient Boosting algorithm shows the best performance with default attributes for this dataset. We can start our analysis our development based in this algorithm.\n",
    "\n",
    "There's a lot work to do, but this is the begining. Thanks for reading. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

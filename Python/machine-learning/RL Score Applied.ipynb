{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Regularized least squares (RLS)](https://en.wikipedia.org/wiki/Regularized_least_squares) is a family of methods for solving the least-squares problem while using regularization to further constrain the resulting solution.\n",
    "\n",
    "RLS is used for two main reasons. The first comes up when the number of variables in the linear system exceeds the number of observations. In such settings, the ordinary least-squares problem is ill-posed and is therefore impossible to fit because the associated optimization problem has infinitely many solutions. RLS allows the introduction of further constraints that uniquely determine the solution.\n",
    "\n",
    "The second reason that RLS is used occurs when the number of variables does not exceed the number of observations, but the learned model suffers from poor generalization. RLS can be used in such cases to improve the generalizability of the model by constraining it at training time. This constraint can either force the solution to be \"sparse\" in some way or to reflect other prior knowledge about the problem such as information about correlations between features. A Bayesian understanding of this can be reached by showing that RLS methods are often equivalent to priors on the solution to the least-squares problem. \n",
    "\n",
    "[To sse in Depth](http://www.mit.edu/~9.520/spring07/Classes/rlsslides.pdf)\n",
    "\n",
    "\n",
    "Installation  \n",
    "1) `$ pip install rlscore`  \n",
    "2) `$ export CFLAGS=\"-I /usr/local/lib/python2.7/site-packages/numpy/core/include $CFLAGS\"`\n",
    "\n",
    "[Original post](http://staff.cs.utu.fi/~aatapa/software/RLScore/tutorial_regression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "from rlscore.learner import RLS\n",
    "from rlscore.measure import sqerror\n",
    "from rlscore.learner import LeaveOneOutRLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Function to load dataset and split in train and test sets\n",
    "def load_housing():\n",
    "    np.random.seed(1)\n",
    "    D = np.loadtxt(\"/Volumes/PANZER/Github/learning-space/Datasets/02 - Classification/housing_data.txt\")\n",
    "    np.random.shuffle(D)\n",
    "    X = D[:,:-1] # Independent variables\n",
    "    Y = D[:,-1]  # Dependent variable\n",
    "    X_train = X[:250]\n",
    "    Y_train = Y[:250]\n",
    "    X_test = X[250:]\n",
    "    Y_test = Y[250:]\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Housing data set characteristics\n",
      "Training set: 250 instances, 13 features\n",
      "Test set: 256 instances, 13 features\n"
     ]
    }
   ],
   "source": [
    "def print_stats():\n",
    "    X_train, Y_train, X_test, Y_test = load_housing()\n",
    "    print(\"Housing data set characteristics\")\n",
    "    print(\"Training set: %d instances, %d features\" %X_train.shape)\n",
    "    print(\"Test set: %d instances, %d features\" %X_test.shape)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leave-one-out error 25.959399\n",
      "test error 25.497222\n",
      "mean predictor 81.458770\n"
     ]
    }
   ],
   "source": [
    "# Function to train RLS method\n",
    "def train_rls():\n",
    "    #Trains RLS with default parameters (regparam=1.0, kernel='LinearKernel')\n",
    "    X_train, Y_train, X_test, Y_test = load_housing()\n",
    "    learner = RLS(X_train, Y_train)\n",
    "    \n",
    "    #Leave-one-out cross-validation predictions, this is fast due to\n",
    "    #computational short-cut\n",
    "    P_loo = learner.leave_one_out()\n",
    "    \n",
    "    #Test set predictions\n",
    "    P_test = learner.predict(X_test)\n",
    "    \n",
    "    # Stats\n",
    "    print(\"leave-one-out error %f\" %sqerror(Y_train, P_loo))\n",
    "    print(\"test error %f\" %sqerror(Y_test, P_test))\n",
    "    \n",
    "    #Sanity check, can we do better than predicting mean of training labels?\n",
    "    print(\"mean predictor %f\" %sqerror(Y_test, np.ones(Y_test.shape)*np.mean(Y_train)))\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    train_rls()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing regularization parameter with leave-one-out\n",
    "\n",
    "Regularization parameter with grid search in exponential grid to catch the lowest LOO-CV error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regparam 2**-15, loo-error 24.745479\n",
      "regparam 2**-14, loo-error 24.745463\n",
      "regparam 2**-13, loo-error 24.745431\n",
      "regparam 2**-12, loo-error 24.745369\n",
      "regparam 2**-11, loo-error 24.745246\n",
      "regparam 2**-10, loo-error 24.745010\n",
      "regparam 2**-9, loo-error 24.744576\n",
      "regparam 2**-8, loo-error 24.743856\n",
      "regparam 2**-7, loo-error 24.742982\n",
      "regparam 2**-6, loo-error 24.743309\n",
      "regparam 2**-5, loo-error 24.750966\n",
      "regparam 2**-4, loo-error 24.786243\n",
      "regparam 2**-3, loo-error 24.896991\n",
      "regparam 2**-2, loo-error 25.146493\n",
      "regparam 2**-1, loo-error 25.537315\n",
      "regparam 2**0, loo-error 25.959399\n",
      "regparam 2**1, loo-error 26.285436\n",
      "regparam 2**2, loo-error 26.479254\n",
      "regparam 2**3, loo-error 26.603001\n",
      "regparam 2**4, loo-error 26.801196\n",
      "regparam 2**5, loo-error 27.352322\n",
      "regparam 2**6, loo-error 28.837002\n",
      "regparam 2**7, loo-error 32.113350\n",
      "regparam 2**8, loo-error 37.480625\n",
      "regparam 2**9, loo-error 43.843555\n",
      "regparam 2**10, loo-error 49.748687\n",
      "regparam 2**11, loo-error 54.912297\n",
      "regparam 2**12, loo-error 59.936226\n",
      "regparam 2**13, loo-error 65.137825\n",
      "regparam 2**14, loo-error 70.126118\n",
      "regparam 2**15, loo-error 74.336978\n",
      "best regparam 0.007812 with loo-error 24.742982\n",
      "test error 24.509981\n"
     ]
    }
   ],
   "source": [
    "def train_rls():\n",
    "    #Select regparam with leave-one-out cross-validation\n",
    "    X_train, Y_train, X_test, Y_test = load_housing()\n",
    "    learner = RLS(X_train, Y_train)\n",
    "    best_regparam = None\n",
    "    best_error = float(\"inf\")\n",
    "   \n",
    "    #exponential grid of possible regparam values\n",
    "    log_regparams = range(-15, 16)\n",
    "    for log_regparam in log_regparams:\n",
    "        regparam = 2.**log_regparam\n",
    "        \n",
    "        #RLS is re-trained with the new regparam, this\n",
    "        #is very fast due to computational short-cut\n",
    "        learner.solve(regparam)\n",
    "        \n",
    "        #Leave-one-out cross-validation predictions, this is fast due to\n",
    "        #computational short-cut\n",
    "        P_loo = learner.leave_one_out()\n",
    "        e = sqerror(Y_train, P_loo)\n",
    "        print(\"regparam 2**%d, loo-error %f\" %(log_regparam, e))\n",
    "        if e < best_error:\n",
    "            best_error = e\n",
    "            best_regparam = regparam\n",
    "    learner.solve(best_regparam)\n",
    "    P_test = learner.predict(X_test)\n",
    "    print(\"best regparam %f with loo-error %f\" %(best_regparam, best_error)) \n",
    "    print(\"test error %f\" %sqerror(Y_test, P_test))\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    train_rls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with RLS and simultaneously selecting the regularization parameter with leave-one-out using LeaveOneOutRLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "leave-one-out errors [ 24.74547881  24.74546295  24.74543138  24.74536884  24.74524616\n",
      "  24.74501033  24.7445764   24.74385625  24.74298177  24.74330936\n",
      "  24.75096639  24.78624255  24.89699067  25.14649266  25.53731465\n",
      "  25.95939943  26.28543584  26.47925431  26.6030015   26.80119588\n",
      "  27.35232186  28.83700156  32.11334986  37.48062503  43.84355496\n",
      "  49.7486873   54.91229746  59.93622566  65.1378248   70.12611801\n",
      "  74.33697809]\n",
      "chosen regparam 0.007812\n",
      "test error 24.509981\n"
     ]
    }
   ],
   "source": [
    "def train_rls():\n",
    "    #Trains RLS with automatically selected regularization parameter\n",
    "    X_train, Y_train, X_test, Y_test = load_housing()\n",
    "    \n",
    "    # Grid search\n",
    "    regparams = [2.**i for i in range(-15, 16)]\n",
    "    learner = LeaveOneOutRLS(X_train, Y_train, regparams = regparams)\n",
    "    loo_errors = learner.cv_performances\n",
    "    P_test = learner.predict(X_test)\n",
    "    print(\"leave-one-out errors \" +str(loo_errors))\n",
    "    print(\"chosen regparam %f\" %learner.regparam)\n",
    "    print(\"test error %f\" %sqerror(Y_test, P_test))\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    train_rls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning nonlinear predictors using kernels\n",
    "\n",
    "RLS using a non-linear kernel function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters gamma 0.000031 regparam 0.000244\n",
      "best leave-one-out error 21.910837\n",
      "test error 16.340877\n"
     ]
    }
   ],
   "source": [
    "def train_rls():\n",
    "    #Selects both the gamma parameter for Gaussian kernel, and regparam with loocv\n",
    "    X_train, Y_train, X_test, Y_test = load_housing()\n",
    "    \n",
    "    regparams = [2.**i for i in range(-15, 16)]\n",
    "    gammas = regparams\n",
    "    best_regparam = None\n",
    "    best_gamma = None\n",
    "    best_error = float(\"inf\")\n",
    "    \n",
    "    for gamma in gammas:\n",
    "        #New RLS is initialized for each kernel parameter\n",
    "        learner = RLS(X_train, Y_train, kernel=\"GaussianKernel\", gamma=gamma)\n",
    "        for regparam in regparams:\n",
    "            #RLS is re-trained with the new regparam, this\n",
    "            #is very fast due to computational short-cut\n",
    "            learner.solve(regparam)\n",
    "            \n",
    "            #Leave-one-out cross-validation predictions, this is fast due to\n",
    "            #computational short-cut\n",
    "            P_loo = learner.leave_one_out()\n",
    "            e = sqerror(Y_train, P_loo)\n",
    "            \n",
    "            #print \"regparam\", regparam, \"gamma\", gamma, \"loo-error\", e\n",
    "            if e < best_error:\n",
    "                best_error = e\n",
    "                best_regparam = regparam\n",
    "                best_gamma = gamma\n",
    "    learner = RLS(X_train, Y_train, regparam = best_regparam, kernel=\"GaussianKernel\", gamma=best_gamma)\n",
    "    P_test = learner.predict(X_test)\n",
    "    print(\"best parameters gamma %f regparam %f\" %(best_gamma, best_regparam))\n",
    "    print(\"best leave-one-out error %f\" %best_error)\n",
    "    print(\"test error %f\" %sqerror(Y_test, P_test))\n",
    "    \n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    train_rls()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary classification and Area under ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adult data set characteristics\n",
      "Training set: 30956 instances, 123 features\n",
      "Test set: 1605 instances, 119 features\n"
     ]
    }
   ],
   "source": [
    "from rlscore.utilities.reader import read_svmlight\n",
    "\n",
    "# Load dataset and stats\n",
    "def print_stats():\n",
    "    X_train, Y_train, foo = read_svmlight(\"/Volumes/PANZER/Github/learning-space/Datasets/02 - Classification/a1a.t\")\n",
    "    X_test, Y_test, foo = read_svmlight(\"/Volumes/PANZER/Github/learning-space/Datasets/02 - Classification/a1a\")\n",
    "    print(\"Adult data set characteristics\")\n",
    "    print(\"Training set: %d instances, %d features\" %X_train.shape)\n",
    "    print(\"Test set: %d instances, %d features\" %X_test.shape)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    print_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from rlscore.learner import RLS\n",
    "from rlscore.measure import accuracy\n",
    "from rlscore.utilities.reader import read_svmlight\n",
    "\n",
    "\n",
    "def train_rls():\n",
    "    # Train ans test datasets    \n",
    "    X_train, Y_train, foo = read_svmlight(\"/Volumes/PANZER/Github/learning-space/Datasets/02 - Classification/a1a.t\")\n",
    "    X_test, Y_test, foo = read_svmlight(\"/Volumes/PANZER/Github/learning-space/Datasets/02 - Classification/a1a\", X_train.shape[1])\n",
    "    learner = RLS(X_train, Y_train)\n",
    "    best_regparam = None\n",
    "    best_accuracy = 0.\n",
    "    \n",
    "    #exponential grid of possible regparam values\n",
    "    log_regparams = range(-15, 16)\n",
    "    for log_regparam in log_regparams:\n",
    "        regparam = 2.**log_regparam\n",
    "        #RLS is re-trained with the new regparam, this\n",
    "        #is very fast due to computational short-cut\n",
    "        learner.solve(regparam)\n",
    "        \n",
    "        #Leave-one-out cross-validation predictions, this is fast due to\n",
    "        #computational short-cut\n",
    "        P_loo = learner.leave_one_out()\n",
    "        acc = accuracy(Y_train, P_loo)\n",
    "        \n",
    "        print(\"regparam 2**%d, loo-accuracy %f\" %(log_regparam, acc))\n",
    "        if acc > best_accuracy:\n",
    "            best_accuracy = acc\n",
    "            best_regparam = regparam\n",
    "    learner.solve(best_regparam)\n",
    "    P_test = learner.predict(X_test)\n",
    "    \n",
    "    print(\"best regparam %f with loo-accuracy %f\" %(best_regparam, best_accuracy)) \n",
    "    print(\"test set accuracy %f\" %accuracy(Y_test, P_test))\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    train_rls()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
